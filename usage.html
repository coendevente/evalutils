
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Usage &#8212; evalutils 0.3.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Modules" href="modules.html" />
    <link rel="prev" title="Installation" href="installation.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="usage">
<h1>Usage<a class="headerlink" href="#usage" title="Permalink to this headline">¶</a></h1>
<section id="getting-started">
<h2>Getting Started<a class="headerlink" href="#getting-started" title="Permalink to this headline">¶</a></h2>
<p>This guide shows you how to use <code class="docutils literal notranslate"><span class="pre">evalutils</span></code> to generate either an
evaluation container or an algorithm container for <a class="reference external" href="https://grand-challenge.org">Grand Challenge</a>.
Select the appropriate project below to get started.</p>
</section>
<section id="evaluation-container">
<h2>Evaluation container<a class="headerlink" href="#evaluation-container" title="Permalink to this headline">¶</a></h2>
<p>This guide will show you how to use <code class="docutils literal notranslate"><span class="pre">evalutils</span></code> to generate an evaluation
container for <a class="reference external" href="https://grand-challenge.org">Grand Challenge</a>. In this example we will call our project
<code class="docutils literal notranslate"><span class="pre">myproject</span></code>, substitute your project name wherever you see this.</p>
<section id="prerequisites">
<h3>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline">¶</a></h3>
<p>Before you start you will need to have:</p>
<ul class="simple">
<li><p>A local <a class="reference external" href="https://www.docker.com/">docker</a> installation</p></li>
<li><p>Your challenge test set ground truth data, this can be a CSV file or a set of images</p></li>
<li><p>An idea about what metrics you want to score the submissions on</p></li>
</ul>
</section>
<section id="generate-the-project-structure">
<h3>Generate The Project Structure<a class="headerlink" href="#generate-the-project-structure" title="Permalink to this headline">¶</a></h3>
<p>Evalutils contains a project generator based on <a class="reference external" href="https://github.com/audreyr/cookiecutter">CookieCutter</a> that you can
use to generate the boilerplate for your evaluation.
Once you have installed evalutils you can see the options with:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>evalutils init evaluation --help
</pre></div>
</div>
<p>Say that you want to create an evaluation for <code class="docutils literal notranslate"><span class="pre">myproject</span></code>, you can initialize
it with:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>evalutils init evaluation myproject
</pre></div>
</div>
<p>You will then be prompted to choose a challenge type:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>What kind of challenge is this? <span class="o">[</span>Classification<span class="p">|</span>Segmentation<span class="p">|</span>Detection<span class="o">]</span>:
</pre></div>
</div>
<p>so type in your challenge type and press &lt;enter&gt;.
The different challenge types that you can select are:</p>
<ul class="simple">
<li><dl class="simple">
<dt><strong>Classification</strong>:</dt><dd><p>The submission and ground truth are csv files with the same number of rows.
For instance, this evaluation could be used for scoring classification of whole images into 1 or multiple classes.
The result of the evaluation is not reported on the case level to prevent leaking of the ground truth data.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Segmentation</strong>:</dt><dd><p>A special case of a classification task, the difference is that the submission and ground truth are image files (eg, ITK images or a collection of PNGs).
For instance, this evaluation could be used for scoring structure segmentation in 3D images.
There are the same number images in the ground truth dataset as there are in each submission.
By default, the results per case are also reported.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Detection</strong>:</dt><dd><p>The submission and ground truth are csv files, but with differing number of rows.
For instance, this evaluation could be used for scoring detection of tumours in images.
For this sort of challenge, you may have many candidate points and many ground truth points per case.
By default, the results per case are also reported.</p>
</dd>
</dl>
</li>
</ul>
<p>If you do not have a local python 3.7+ environment you can also
generate your project with docker by running a container and sharing your current user id:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker run -it --rm -u <span class="sb">`</span>id -u<span class="sb">`</span> -v <span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/usr/src/myapp -w /usr/src/myapp python:3 bash -c <span class="s2">&quot;pip install evalutils &amp;&amp; evalutils init evaluation myproject&quot;</span>
</pre></div>
</div>
<p>Either of these commands will generate a folder called <code class="docutils literal notranslate"><span class="pre">myproject</span></code>
with everything you need to get started.</p>
<p>It is a good idea to commit your project to git right now. You can do this with:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span><span class="nb">cd</span> myproject
<span class="gp">$ </span>git init
<span class="gp">$ </span>git lfs install   <span class="o">(</span>see the warning below<span class="o">)</span>
<span class="gp">$ </span>git add --all
<span class="gp">$ </span>git commit -m <span class="s2">&quot;Initial Commit&quot;</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The test set ground truth will be stored in this repo,
so remember to use a private repo if you’re going to push this to github or gitlab,
and use <a class="reference external" href="https://git-lfs.github.com/">git lfs</a> if your ground truth data are large.</p>
<p>The .gitattributes file at the root of the repository specifies all the files which should be
tracked by git-lfs. By default all files in the ground truth and test directories
are configured to be tracked by git-lfs, but they will only be registered
once the <a class="reference external" href="https://git-lfs.github.com/">git lfs</a> extension is installed on your system and the <code class="docutils literal notranslate"><span class="pre">git</span> <span class="pre">lfs</span> <span class="pre">install</span></code>
command has been issued inside the generated repository.</p>
</div>
<p>The structure of the project will be:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">.</span>
<span class="go">└── myproject</span>
<span class="go">    ├── build.sh            # Builds your evaluation container</span>
<span class="go">    ├── Dockerfile          # Defines how to build your evaluation container</span>
<span class="go">    ├── evaluation.py       # Contains your evaluation code - this is where you will extend the Evaluation class</span>
<span class="go">    ├── export.sh           # Exports your container to a .tar file for use on grand-challenge.org</span>
<span class="go">    ├── .gitattributes      # Define which files git should put under git-lfs</span>
<span class="go">    ├── .gitignore          # Define which files git should ignore</span>
<span class="go">    ├── ground-truth        # A folder that contains your ground truth annotations</span>
<span class="go">    │   └── reference.csv   # In this example the ground truth is a csv file</span>
<span class="go">    ├── README.md           # For describing your evaluation to others</span>
<span class="go">    ├── requirements.txt    # The python dependencies of your evaluation container - add any new dependencies here</span>
<span class="go">    ├── test                # A folder that contains an example submission for testing</span>
<span class="go">    │   └── submission.csv  # In this example the participants will submit a csv file</span>
<span class="go">    └── test.sh             # A script that runs your evaluation container on the test submission</span>
</pre></div>
</div>
<p>For Segmentation tasks, some example mhd/zraw files will be in the ground-truth and test directories instead.</p>
<p>The most important file is <code class="docutils literal notranslate"><span class="pre">evaluation.py</span></code>.
This is the file where you will extend the <code class="docutils literal notranslate"><span class="pre">Evaluation</span></code> class and implement the evaluation for your challenge.
In this file, a new class has been created for you, and it is instantiated and run with:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">Myproject</span><span class="p">()</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
</pre></div>
</div>
<p>This is all that is needed for <code class="docutils literal notranslate"><span class="pre">evalutils</span></code> to perform the evaluation and generate the output for each new submission.
The superclass of <code class="docutils literal notranslate"><span class="pre">Evaluation</span></code> is what you need to adapt to your specific challenge.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When designing your submission format do NOT use subfolders to differentiate between tasks.
If you have multiple tasks, you should be using multiple phases and evaluations instead.
On grand challenge all common path prefixes will be removed from the submitted files,
so your evaluation should only work with flat lists of files.</p>
</div>
<section id="classification-tasks">
<h4>Classification Tasks<a class="headerlink" href="#classification-tasks" title="Permalink to this headline">¶</a></h4>
<p>The boilerplate for classification challenges looks like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Myproject</span><span class="p">(</span><span class="n">ClassificationEvaluation</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">file_loader</span><span class="o">=</span><span class="n">CSVLoader</span><span class="p">(),</span>
            <span class="n">validators</span><span class="o">=</span><span class="p">(</span>
                <span class="n">ExpectedColumnNamesValidator</span><span class="p">(</span><span class="n">expected</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;case&quot;</span><span class="p">,</span> <span class="s2">&quot;class&quot;</span><span class="p">,)),</span>
                <span class="n">NumberOfCasesValidator</span><span class="p">(</span><span class="n">num_cases</span><span class="o">=</span><span class="mi">8</span><span class="p">),</span>
            <span class="p">),</span>
            <span class="n">join_key</span><span class="o">=</span><span class="s2">&quot;case&quot;</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">score_aggregates</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;accuracy_score&quot;</span><span class="p">:</span> <span class="n">accuracy_score</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_cases</span><span class="p">[</span><span class="s2">&quot;class_ground_truth&quot;</span><span class="p">],</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_cases</span><span class="p">[</span><span class="s2">&quot;class_prediction&quot;</span><span class="p">],</span>
             <span class="p">),</span>
        <span class="p">}</span>
</pre></div>
</div>
<p>In this case the evaluation is loading csv files, so uses an instance <code class="docutils literal notranslate"><span class="pre">CSVLoader</span></code> which will do the loading of the data.
In this example, both the ground truth and the prediction CSV files will contain the columns <cite>case</cite> (an index) and <cite>class</cite> (the predicted class of this case).
We want to validate that the correct columns appear in both the ground truth and submitted predictions, so we use the <code class="docutils literal notranslate"><span class="pre">ExpectedColumnNamesValidator</span></code> with the names of the columns we expect to find.
We also use the <code class="docutils literal notranslate"><span class="pre">NumberOfCasesValidator</span></code> to check that the correct number of cases has been submitted by the challenge participant.
See <a class="reference internal" href="modules.html#module-evalutils.validators" title="evalutils.validators"><code class="xref py py-mod docutils literal notranslate"><span class="pre">evalutils.validators</span></code></a> for a list of other validators that you can use.</p>
<p>The ground truth and predictions will be loaded into two DataFrames.
The last argument is a <code class="docutils literal notranslate"><span class="pre">join_key</span></code>, the is the name of the column that will appear in both DataFrames that serves as an index to join the dataframes on in order to create <code class="docutils literal notranslate"><span class="pre">self._cases</span></code>.
The <code class="docutils literal notranslate"><span class="pre">join_key</span></code> is manditory when you use a <code class="docutils literal notranslate"><span class="pre">CSVLoader</span></code>.
This should be set to some sort of common index, such as a <cite>case</cite> identifier.
When loading in files they are first going to be sorted so you might not need a <code class="docutils literal notranslate"><span class="pre">join_key</span></code>, but you could also write a function that matches the cases based on filename.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>It is best practice to include an integer in the (file) name that uniquely defines each case.
For instance, name your testing set files case_001, case_002, … etc.</p>
</div>
<p>The last part is performing the actual evaluation.
In this example we are only getting one number per submission, the accuracy score.
This number is calculated using <code class="docutils literal notranslate"><span class="pre">sklearn.metrics.accuracy_score</span></code>.
The <code class="docutils literal notranslate"><span class="pre">self._cases</span></code> data frame will contain all of the columns that you expect, and for those that have not been joined they will be available as <code class="docutils literal notranslate"><span class="pre">&quot;&lt;column_name&gt;_ground_truth&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">&quot;&lt;column_name&gt;_prediction&quot;</span></code>.</p>
<p>If you need to score cases individually before aggregating them, you should remove the implementation of <code class="docutils literal notranslate"><span class="pre">score_aggregates</span></code> and implement <code class="docutils literal notranslate"><span class="pre">score_case</span></code> instead.</p>
</section>
<section id="segmentation-tasks">
<h4>Segmentation Tasks<a class="headerlink" href="#segmentation-tasks" title="Permalink to this headline">¶</a></h4>
<p>For segmentation tasks, the generated code will look like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Myproject</span><span class="p">(</span><span class="n">ClassificationEvaluation</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">file_loader</span><span class="o">=</span><span class="n">SimpleITKLoader</span><span class="p">(),</span>
            <span class="n">validators</span><span class="o">=</span><span class="p">(</span>
                <span class="n">NumberOfCasesValidator</span><span class="p">(</span><span class="n">num_cases</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
                <span class="n">UniquePathIndicesValidator</span><span class="p">(),</span>
                <span class="n">UniqueImagesValidator</span><span class="p">(),</span>
            <span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">score_case</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">case</span><span class="p">):</span>
        <span class="n">gt_path</span> <span class="o">=</span> <span class="n">case</span><span class="p">[</span><span class="s2">&quot;path_ground_truth&quot;</span><span class="p">]</span>
        <span class="n">pred_path</span> <span class="o">=</span> <span class="n">case</span><span class="p">[</span><span class="s2">&quot;path_prediction&quot;</span><span class="p">]</span>

        <span class="c1"># Load the images for this case</span>
        <span class="n">gt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_file_loader</span><span class="o">.</span><span class="n">load_image</span><span class="p">(</span><span class="n">gt_path</span><span class="p">)</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_file_loader</span><span class="o">.</span><span class="n">load_image</span><span class="p">(</span><span class="n">pred_path</span><span class="p">)</span>

        <span class="c1"># Check that they&#39;re the right images</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_file_loader</span><span class="o">.</span><span class="n">hash_image</span><span class="p">(</span><span class="n">gt</span><span class="p">)</span> <span class="o">==</span> <span class="n">case</span><span class="p">[</span><span class="s2">&quot;hash_ground_truth&quot;</span><span class="p">]</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_file_loader</span><span class="o">.</span><span class="n">hash_image</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span> <span class="o">==</span> <span class="n">case</span><span class="p">[</span><span class="s2">&quot;hash_prediction&quot;</span><span class="p">]</span>

        <span class="c1"># Cast to the same type</span>
        <span class="n">caster</span> <span class="o">=</span> <span class="n">SimpleITK</span><span class="o">.</span><span class="n">CastImageFilter</span><span class="p">()</span>
        <span class="n">caster</span><span class="o">.</span><span class="n">SetOutputPixelType</span><span class="p">(</span><span class="n">SimpleITK</span><span class="o">.</span><span class="n">sitkUInt8</span><span class="p">)</span>
        <span class="n">gt</span> <span class="o">=</span> <span class="n">caster</span><span class="o">.</span><span class="n">Execute</span><span class="p">(</span><span class="n">gt</span><span class="p">)</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">caster</span><span class="o">.</span><span class="n">Execute</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>

        <span class="c1"># Score the case</span>
        <span class="n">overlap_measures</span> <span class="o">=</span> <span class="n">SimpleITK</span><span class="o">.</span><span class="n">LabelOverlapMeasuresImageFilter</span><span class="p">()</span>
        <span class="n">overlap_measures</span><span class="o">.</span><span class="n">Execute</span><span class="p">(</span><span class="n">gt</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;FalseNegativeError&#39;</span><span class="p">:</span> <span class="n">overlap_measures</span><span class="o">.</span><span class="n">GetFalseNegativeError</span><span class="p">(),</span>
            <span class="s1">&#39;FalsePositiveError&#39;</span><span class="p">:</span> <span class="n">overlap_measures</span><span class="o">.</span><span class="n">GetFalsePositiveError</span><span class="p">(),</span>
            <span class="s1">&#39;MeanOverlap&#39;</span><span class="p">:</span> <span class="n">overlap_measures</span><span class="o">.</span><span class="n">GetMeanOverlap</span><span class="p">(),</span>
            <span class="s1">&#39;UnionOverlap&#39;</span><span class="p">:</span> <span class="n">overlap_measures</span><span class="o">.</span><span class="n">GetUnionOverlap</span><span class="p">(),</span>
            <span class="s1">&#39;VolumeSimilarity&#39;</span><span class="p">:</span> <span class="n">overlap_measures</span><span class="o">.</span><span class="n">GetVolumeSimilarity</span><span class="p">(),</span>
            <span class="s1">&#39;JaccardCoefficient&#39;</span><span class="p">:</span> <span class="n">overlap_measures</span><span class="o">.</span><span class="n">GetJaccardCoefficient</span><span class="p">(),</span>
            <span class="s1">&#39;DiceCoefficient&#39;</span><span class="p">:</span> <span class="n">overlap_measures</span><span class="o">.</span><span class="n">GetDiceCoefficient</span><span class="p">(),</span>
            <span class="s1">&#39;pred_fname&#39;</span><span class="p">:</span> <span class="n">pred_path</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
            <span class="s1">&#39;gt_fname&#39;</span><span class="p">:</span> <span class="n">gt_path</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
        <span class="p">}</span>
</pre></div>
</div>
<p>Here, we are loading ITK files in the ground-truth and test folders using <code class="docutils literal notranslate"><span class="pre">SimpleITKLoader</span></code>.
See <a class="reference internal" href="modules.html#module-evalutils.io" title="evalutils.io"><code class="xref py py-mod docutils literal notranslate"><span class="pre">evalutils.io</span></code></a> for the other image loaders you could use.
By default, the files will be matched together based on the first integer found in the filename, so name your ground truth files, for example, case_001.mha, case_002.mha, etc.
Have the participants for your challenge do the same.</p>
<p>The loader will try to load all of the files in the ground-truth and submission folders.
To check that the correct number of images were submitted by the participant and loaded we use <code class="docutils literal notranslate"><span class="pre">NumberOfCasesValidator</span></code>, and check that the images are unique by using <code class="docutils literal notranslate"><span class="pre">UniquePathIndicesValidator</span></code> and <code class="docutils literal notranslate"><span class="pre">UniqueImagesValidator</span></code></p>
<p>The <code class="docutils literal notranslate"><span class="pre">score_case</span></code> function will calculate the score for each case, in this case we’re calculating some overlap measures using <code class="docutils literal notranslate"><span class="pre">SimpleITK</span></code>.
The images are not stored in the case dataframe to save memory, so first they are loaded using the file loader, and are then checked that they are the valid images by calculating the hash.
The filenames are also stored for the case for matching later on grand-challenge.</p>
<p>The aggregate results are automatically calculated using <code class="docutils literal notranslate"><span class="pre">score_aggregates</span></code>, which calls <code class="docutils literal notranslate"><span class="pre">DataFrame.describe()</span></code>.
By default, this will calculate the mean, quartile ranges and counts of each individual metric.</p>
</section>
<section id="detection-tasks">
<h4>Detection Tasks<a class="headerlink" href="#detection-tasks" title="Permalink to this headline">¶</a></h4>
<p>The generated boilerplate for detection tasks is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Myproject</span><span class="p">(</span><span class="n">DetectionEvaluation</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">file_loader</span><span class="o">=</span><span class="n">CSVLoader</span><span class="p">(),</span>
            <span class="n">validators</span><span class="o">=</span><span class="p">(</span>
                <span class="n">ExpectedColumnNamesValidator</span><span class="p">(</span>
                    <span class="n">expected</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;image_id&quot;</span><span class="p">,</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="s2">&quot;score&quot;</span><span class="p">)</span>
                <span class="p">),</span>
            <span class="p">),</span>
            <span class="n">join_key</span><span class="o">=</span><span class="s2">&quot;image_id&quot;</span><span class="p">,</span>
            <span class="n">detection_radius</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
            <span class="n">detection_threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_points</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">case</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts the set of ground truth or predictions for this case, into</span>
<span class="sd">        points that represent true positives or predictions</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">points</span> <span class="o">=</span> <span class="n">case</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="c1"># There are no ground truth/prediction points for this case</span>
            <span class="k">return</span> <span class="p">[]</span>

        <span class="k">return</span> <span class="p">[</span>
            <span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span> <span class="n">p</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">points</span><span class="o">.</span><span class="n">iterrows</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">p</span><span class="p">[</span><span class="s2">&quot;score&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_detection_threshold</span>
        <span class="p">]</span>
</pre></div>
</div>
<p>In this case, we are loading a CSV file with <code class="docutils literal notranslate"><span class="pre">CSVLoader</span></code>, but do not validate the number of rows as they can be different between the ground truth and submissions.
We validate the column headers in both files.
In this case, we identify the cases with <code class="docutils literal notranslate"><span class="pre">image_id</span></code>, and both files contain <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> locations, with a confidence score of <code class="docutils literal notranslate"><span class="pre">score</span></code>.
In the ground truth dataset the score should be set to 1.</p>
<p>By default, The predictions will be thresholded at <code class="docutils literal notranslate"><span class="pre">detection_threshold</span></code>.
The detection evaluation will count the closest prediction that lies within distance <code class="docutils literal notranslate"><span class="pre">detection_radius</span></code> from the ground truth point as a true positive.
See <a class="reference internal" href="modules.html#module-evalutils.scorers" title="evalutils.scorers"><code class="xref py py-mod docutils literal notranslate"><span class="pre">evalutils.scorers</span></code></a> for more information on the algorithm.</p>
<p>The only function that needs to be implemented is <code class="docutils literal notranslate"><span class="pre">get_points</span></code>, which converts a case row to a list of points which are later matched.
In this case, we’re acting on 2D images, but you could extend <code class="docutils literal notranslate"><span class="pre">(p[&quot;x&quot;],</span> <span class="pre">p[&quot;y&quot;])</span></code> to say <code class="docutils literal notranslate"><span class="pre">(p[&quot;x&quot;],</span> <span class="pre">p[&quot;y&quot;],</span> <span class="pre">p[&quot;z&quot;])</span></code> if you have 3D data.</p>
<p>By default, the f1 score, precision and accuracy are calculated for each case, see the <code class="docutils literal notranslate"><span class="pre">DetectionEvaluation</span></code> class for more information.</p>
</section>
</section>
<section id="add-the-ground-truth-and-test-data">
<h3>Add The Ground Truth and Test Data<a class="headerlink" href="#add-the-ground-truth-and-test-data" title="Permalink to this headline">¶</a></h3>
<p>The next step is to add your ground truth and test data (an example submission) to the repo.
If using CSV data simply update the <code class="docutils literal notranslate"><span class="pre">ground-truth/reference.csv</span></code> file, and then update the expected column names and join key in evaluate.py.
Otherwise, see <a class="reference internal" href="modules.html#module-evalutils.io" title="evalutils.io"><code class="xref py py-mod docutils literal notranslate"><span class="pre">evalutils.io</span></code></a> for other loaders such as the ones for ITK files or images.
You can also add your own loader by extending the <code class="docutils literal notranslate"><span class="pre">FileLoader</span></code> class.</p>
</section>
<section id="adapt-the-evaluation">
<h3>Adapt The Evaluation<a class="headerlink" href="#adapt-the-evaluation" title="Permalink to this headline">¶</a></h3>
<p>Change the function in the boilerplate to fit your needs, refer to the superclass methods for more information on return types.
See <code class="xref py py-class docutils literal notranslate"><span class="pre">evalutils.Evaluation</span></code> for more possibilities.</p>
</section>
<section id="build-test-and-export">
<h3>Build, Test and Export<a class="headerlink" href="#build-test-and-export" title="Permalink to this headline">¶</a></h3>
<p>When you’re ready to test your evaluation you can simply invoke</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>./test.sh
</pre></div>
</div>
<p>This will build your docker container, add the test data as a temporary volume, run the evaluation, and then <code class="docutils literal notranslate"><span class="pre">cat</span> <span class="pre">/output/metrics.json</span></code>.
If the output looks ok, then you’re ready to go.</p>
<p>You can export the evaluation container with</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>./export.sh
</pre></div>
</div>
<p>which will create myproject.tar in the folder.
You can then upload this directly to <a class="reference external" href="https://grand-challenge.org">Grand Challenge</a> on your evaluation methods page.</p>
</section>
</section>
<section id="algorithm-container">
<h2>Algorithm container<a class="headerlink" href="#algorithm-container" title="Permalink to this headline">¶</a></h2>
<p>This guide will show you how to use <code class="docutils literal notranslate"><span class="pre">evalutils</span></code> to generate an algorithm
container for <a class="reference external" href="https://grand-challenge.org">Grand Challenge</a>. In this example we will call our project
<code class="docutils literal notranslate"><span class="pre">myproject</span></code>, substitute your project name wherever you see this.</p>
<section id="id1">
<h3>Prerequisites<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>Before you start you will need to have:</p>
<ul class="simple">
<li><p>A local <a class="reference external" href="https://www.docker.com/">docker</a> installation</p></li>
<li><p>Some test images for your algorithm, preferably with expected output</p></li>
</ul>
</section>
<section id="id2">
<h3>Generate The Project Structure<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>Evalutils contains a project generator based on <a class="reference external" href="https://github.com/audreyr/cookiecutter">CookieCutter</a> that you can
use to generate the boilerplate for your algorithm.
Once you have installed evalutils you can see the options with:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>evalutils init algorithm --help
</pre></div>
</div>
<p>Say that you want to create an evaluation for <code class="docutils literal notranslate"><span class="pre">myproject</span></code>, you can initialize
it with:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>evalutils init algorithm myproject
</pre></div>
</div>
<p>You will then be prompted to choose an algorithm type:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>What kind of algorithm is this? <span class="o">[</span>Classification<span class="p">|</span>Segmentation<span class="p">|</span>Detection<span class="o">]</span>:
</pre></div>
</div>
<p>so type in your algorithm type and press &lt;enter&gt;.
The different algorithm types that you can select are:</p>
<ul class="simple">
<li><dl class="simple">
<dt><strong>Classification</strong>:</dt><dd><p>This type of algorithm takes in an image (eg, ITK images or a collection of PNGs) and outputs a <cite>/output/results.json</cite> file.
For instance, this algorithm could be used for classification of whole images into 1 or multiple classes.
By default, the algorithm outputs a single <cite>/output/results.json</cite> which lists the results per case.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Segmentation</strong>:</dt><dd><p>A special case of a classification task, that takes in an image and outputs an image file to <cite>/output/images/</cite> (eg, ITK images or a collection of PNGs).
For instance, this algorithm could be used for structure segmentation in 3D images.
By default, the algorithm outputs an image file at <cite>/output/images/</cite> per case and a single <cite>/output/results.json</cite> file with additional information for all cases.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Detection</strong>:</dt><dd><p>This type of algorithm detects one or more candidates in an image and returns the positions relative to the image.
For instance, this evaluation could be used for detection of tumours in images.
By default, the algorithm outputs a single <cite>/output/results.json</cite> which lists the results per case.</p>
</dd>
</dl>
</li>
</ul>
<p>If you do not have a local python 3.7+ environment you can also
generate your project with docker by running a container and sharing your current user id:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker run -it --rm -u <span class="sb">`</span>id -u<span class="sb">`</span> -v <span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/usr/src/myapp -w /usr/src/myapp python:3 bash -c <span class="s2">&quot;pip install evalutils &amp;&amp; evalutils init algorithm myproject&quot;</span>
</pre></div>
</div>
<p>Either of these commands will generate a folder called <code class="docutils literal notranslate"><span class="pre">myproject</span></code>
with everything you need to get started.</p>
<p>It is a good idea to commit your project to git right now. You can do this with:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span><span class="nb">cd</span> myproject
<span class="gp">$ </span>git init
<span class="gp">$ </span>git lfs install   <span class="o">(</span>see the warning below<span class="o">)</span>
<span class="gp">$ </span>git add --all
<span class="gp">$ </span>git commit -m <span class="s2">&quot;Initial Commit&quot;</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The test input images and the expected output will be stored in this repo,
so remember to use a private repo if you’re going to push this to github or gitlab,
and use <a class="reference external" href="https://git-lfs.github.com/">git lfs</a> if your ground truth data are large.</p>
<p>The .gitattributes file at the root of the repository specifies all the files which should be
tracked by git-lfs. By default all files in the test directories
are configured to be tracked by git-lfs, but they will only be registered
once the <a class="reference external" href="https://git-lfs.github.com/">git lfs</a> extension is installed on your system and the <code class="docutils literal notranslate"><span class="pre">git</span> <span class="pre">lfs</span> <span class="pre">install</span></code>
command has been issued inside the generated repository.</p>
</div>
<p>The structure of the project will be:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">.</span>
<span class="go">└── myproject</span>
<span class="go">    ├── build.sh                 # Builds your algorithm container</span>
<span class="go">    ├── Dockerfile               # Defines how to build your algorithm container</span>
<span class="go">    ├── export.sh                # Exports your algorithm container to a .tar file for use on grand-challenge.org</span>
<span class="go">    ├── .gitattributes           # Define which files git should put under git-lfs</span>
<span class="go">    ├── .github/workflows/ci.yml # Contains a CI configuration file for github workflows for your project</span>
<span class="go">    ├── .gitignore               # Define which files git should ignore</span>
<span class="go">    ├── process.py               # Contains your algorithm code - this is where you will extend the BaseAlgorithm class</span>
<span class="go">    ├── README.md                # For describing your algorithm to others</span>
<span class="go">    ├── requirements.txt         # The python dependencies of your algorithm container - add any new dependencies here</span>
<span class="go">    ├── test                     # A folder that contains an example test image for testing</span>
<span class="go">    │   ├── 1.0.000.000000*.mhd  # An example test image metaio header file</span>
<span class="go">    │   ├── 1.0.000.000000*.zraw # An example test image metaio data file</span>
<span class="go">    │   └── expected_output.json # Output file expected to be produced by the algorithm container</span>
<span class="go">    └── test.sh                  # A script that runs your algorithm container using the example test image and validates the output</span>
</pre></div>
</div>
<p>The most important file is <code class="docutils literal notranslate"><span class="pre">process.py</span></code>.
This is the file where you will extend the <code class="docutils literal notranslate"><span class="pre">Algorithm</span></code> class and implement your algorithm.
In this file, a new class has been created for you, and it is instantiated and run with:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">Myproject</span><span class="p">()</span><span class="o">.</span><span class="n">process</span><span class="p">()</span>
</pre></div>
</div>
<p>This is all that is needed for <code class="docutils literal notranslate"><span class="pre">evalutils</span></code> to run the algorithm and process input images.
The subclass of <code class="docutils literal notranslate"><span class="pre">Algorithm</span></code> is what you need to modify for your specific algorithms.</p>
<p>By default all algorithms will try to load all files in the /input directory using <code class="docutils literal notranslate"><span class="pre">SimpleITKLoader</span></code> for loading the data.
After successfully loading a single image a <code class="docutils literal notranslate"><span class="pre">SimpleITK.Image</span></code> object is ready to be manipulated by the algorithms in
the <code class="docutils literal notranslate"><span class="pre">predict</span></code> method for algorithm tasks like classification, segmentation, or detection.</p>
<section id="classification-algorithm">
<h4>Classification Algorithm<a class="headerlink" href="#classification-algorithm" title="Permalink to this headline">¶</a></h4>
<p>The boilerplate for classification algorithms looks like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Myproject</span><span class="p">(</span><span class="n">ClassificationAlgorithm</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">validators</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
                <span class="n">input_image</span><span class="o">=</span><span class="p">(</span>
                    <span class="n">UniqueImagesValidator</span><span class="p">(),</span>
                    <span class="n">UniquePathIndicesValidator</span><span class="p">(),</span>
                <span class="p">)</span>
            <span class="p">),</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">input_image</span><span class="p">:</span> <span class="n">SimpleITK</span><span class="o">.</span><span class="n">Image</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
        <span class="c1"># Checks if there are any nodules voxels (&gt; 1) in the input image</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;values_exceeding_one&quot;</span><span class="p">:</span> <span class="nb">bool</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">SimpleITK</span><span class="o">.</span><span class="n">GetArrayFromImage</span><span class="p">(</span><span class="n">input_image</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">))</span>
        <span class="p">}</span>
</pre></div>
</div>
<p>The input images can be validated by specifying validators. Here, we want to validate that the input images are unique,
so we use the <code class="docutils literal notranslate"><span class="pre">UniqueImagesValidator</span></code> and <code class="docutils literal notranslate"><span class="pre">UniquePathIndicesValidator</span></code>.
See <a class="reference internal" href="modules.html#module-evalutils.validators" title="evalutils.validators"><code class="xref py py-mod docutils literal notranslate"><span class="pre">evalutils.validators</span></code></a> for a list of other validators that you can use.</p>
<p>All that needs to be modified is the <code class="docutils literal notranslate"><span class="pre">predict</span></code> method of your Algorithm class. By default, it takes an input <code class="docutils literal notranslate"><span class="pre">SimpleITK.Image</span></code>
and expects a dictionary that contains some values based on the image.
In this example it takes the input image, converts it to a <code class="docutils literal notranslate"><span class="pre">numpy.ndarray</span></code>, checks if there is any value larger than 1 in the image, and writes the boolean result to a dictionary.</p>
<p>The output dictionary can have an arbitrary number of key/value pairs. Extending the outputs can be done by adding new dictionary keys and associated values.</p>
<p>The resulting dictionary will be written to <code class="docutils literal notranslate"><span class="pre">/output/results.json</span></code> output file by the super class after running the <code class="docutils literal notranslate"><span class="pre">predict</span></code> method on all inputs.</p>
</section>
<section id="segmentation-algorithm">
<h4>Segmentation Algorithm<a class="headerlink" href="#segmentation-algorithm" title="Permalink to this headline">¶</a></h4>
<p>For segmentation tasks, the generated code will look like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Myproject</span><span class="p">(</span><span class="n">SegmentationAlgorithm</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">validators</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
                <span class="n">input_image</span><span class="o">=</span><span class="p">(</span>
                    <span class="n">UniqueImagesValidator</span><span class="p">(),</span>
                    <span class="n">UniquePathIndicesValidator</span><span class="p">(),</span>
                <span class="p">)</span>
            <span class="p">),</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">input_image</span><span class="p">:</span> <span class="n">SimpleITK</span><span class="o">.</span><span class="n">Image</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SimpleITK</span><span class="o">.</span><span class="n">Image</span><span class="p">:</span>
        <span class="c1"># Segment all values greater than 2 in the input image</span>
        <span class="k">return</span> <span class="n">SimpleITK</span><span class="o">.</span><span class="n">BinaryThreshold</span><span class="p">(</span>
            <span class="n">image1</span><span class="o">=</span><span class="n">input_image</span><span class="p">,</span> <span class="n">lowerThreshold</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">insideValue</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">outsideValue</span><span class="o">=</span><span class="mi">0</span>
        <span class="p">)</span>
</pre></div>
</div>
<p>Similar as before, all that needs to be modified is the <code class="docutils literal notranslate"><span class="pre">predict</span></code> method of your Algorithm class. By default, it takes an input <code class="docutils literal notranslate"><span class="pre">SimpleITK.Image</span></code>
and expects another <code class="docutils literal notranslate"><span class="pre">SimpleITK.Image</span></code> as an output.</p>
<p>In this example it takes the input image, thresholds this for all values greater than or equal to 2 and returns the resulting image.</p>
<p>Besides the default <code class="docutils literal notranslate"><span class="pre">/output/results.json</span></code> output file the SegmentationAlgorithm outputs the resulting images at: <code class="docutils literal notranslate"><span class="pre">/output/images/</span></code>.</p>
</section>
<section id="detection-algorithm">
<h4>Detection Algorithm<a class="headerlink" href="#detection-algorithm" title="Permalink to this headline">¶</a></h4>
<p>The generated boilerplate for detection tasks is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Myproject</span><span class="p">(</span><span class="n">DetectionAlgorithm</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">validators</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
                <span class="n">input_image</span><span class="o">=</span><span class="p">(</span>
                    <span class="n">UniqueImagesValidator</span><span class="p">(),</span>
                    <span class="n">UniquePathIndicesValidator</span><span class="p">(),</span>
                <span class="p">)</span>
            <span class="p">),</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">input_image</span><span class="p">:</span> <span class="n">SimpleITK</span><span class="o">.</span><span class="n">Image</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataFrame</span><span class="p">:</span>
        <span class="c1"># Extract a numpy array with image data from the SimpleITK Image</span>
        <span class="n">image_data</span> <span class="o">=</span> <span class="n">SimpleITK</span><span class="o">.</span><span class="n">GetArrayFromImage</span><span class="p">(</span><span class="n">input_image</span><span class="p">)</span>

        <span class="c1"># Detection: Compute connected components of the maximum values</span>
        <span class="c1"># in the input image and compute their center of mass</span>
        <span class="n">sample_mask</span> <span class="o">=</span> <span class="n">image_data</span> <span class="o">&gt;=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">image_data</span><span class="p">)</span>
        <span class="n">labels</span><span class="p">,</span> <span class="n">num_labels</span> <span class="o">=</span> <span class="n">label</span><span class="p">(</span><span class="n">sample_mask</span><span class="p">)</span>
        <span class="n">candidates</span> <span class="o">=</span> <span class="n">center_of_mass</span><span class="p">(</span>
            <span class="nb">input</span><span class="o">=</span><span class="n">sample_mask</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_labels</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="p">)</span>

        <span class="c1"># Scoring: Score each candidate cluster with the value at its center</span>
        <span class="n">candidate_scores</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">image_data</span><span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">coord</span><span class="p">)]</span>
            <span class="k">for</span> <span class="n">coord</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">candidates</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint16</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="c1"># Serialize candidates and scores as a list of dictionary entries</span>
        <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_serialize_candidates</span><span class="p">(</span>
            <span class="n">candidates</span><span class="o">=</span><span class="n">candidates</span><span class="p">,</span>
            <span class="n">candidate_scores</span><span class="o">=</span><span class="n">candidate_scores</span><span class="p">,</span>
            <span class="n">ref_image</span><span class="o">=</span><span class="n">input_image</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Convert serialized candidates to a pandas.DataFrame</span>
        <span class="k">return</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<p>The only function that needs to be implemented is <code class="docutils literal notranslate"><span class="pre">predict</span></code>, which should extract a list of candidate points from the input image and
return the candidates with some associated scores or labels to a <code class="docutils literal notranslate"><span class="pre">pandas.DataFrame</span></code>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">_serialize_candidates</span></code> helper function takes in a list of candidates in image coordinate space and converts these to world coordinates given a reference <code class="docutils literal notranslate"><span class="pre">SimpleItk.Image</span></code>.
Additionally, the function adds scores per candidate.</p>
<p>The resulting DataFrame is added to the <code class="docutils literal notranslate"><span class="pre">/output/results.json</span></code>.</p>
</section>
</section>
<section id="add-the-test-data-and-the-expected-output-file">
<h3>Add The Test Data And The Expected Output File<a class="headerlink" href="#add-the-test-data-and-the-expected-output-file" title="Permalink to this headline">¶</a></h3>
<p>The next step is to add your test data (an example image and a json file with the expected output) to the repo.
The test images go into the <code class="docutils literal notranslate"><span class="pre">test</span></code> folder in your repo.
To update the expected output simply update the <code class="docutils literal notranslate"><span class="pre">test/expected_output.json</span></code> file.</p>
</section>
<section id="adapt-the-algorithm">
<h3>Adapt The Algorithm<a class="headerlink" href="#adapt-the-algorithm" title="Permalink to this headline">¶</a></h3>
<p>Change the function in the boilerplate to fit your needs, refer to the superclass methods for more information on return types.
See <code class="xref py py-class docutils literal notranslate"><span class="pre">evalutils.BaseAlgorithm</span></code> for more possibilities.</p>
</section>
<section id="id3">
<h3>Build, Test and Export<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>When you’re ready to test your algorithm you can simply invoke</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>./test.sh
</pre></div>
</div>
<p>This will build your docker container, add the test data as a temporary volume, run the algorithm, <code class="docutils literal notranslate"><span class="pre">cat</span> <span class="pre">/output/results.json</span></code>,
and test if the <code class="docutils literal notranslate"><span class="pre">/output/results.json</span></code> matches <code class="docutils literal notranslate"><span class="pre">expected_output.json</span></code> in the test folder of <code class="docutils literal notranslate"><span class="pre">myproject</span></code>.
If the output looks ok and prints <code class="docutils literal notranslate"><span class="pre">Tests</span> <span class="pre">successfully</span> <span class="pre">passed...</span></code>, then you’re ready to go.</p>
<p>You can export the algorithm container with</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>./export.sh
</pre></div>
</div>
<p>which will create <code class="docutils literal notranslate"><span class="pre">myproject.tar.gz</span></code> in the folder.
You can then upload this directly to <a class="reference external" href="https://grand-challenge.org">Grand Challenge</a> on your algorithms page.</p>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">evalutils</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Usage</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#getting-started">Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="#evaluation-container">Evaluation container</a></li>
<li class="toctree-l2"><a class="reference internal" href="#algorithm-container">Algorithm container</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="installation.html" title="previous chapter">Installation</a></li>
      <li>Next: <a href="modules.html" title="next chapter">Modules</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2018, James Meakin.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.5.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/usage.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>